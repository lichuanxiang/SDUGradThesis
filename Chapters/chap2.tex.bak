\chapter{\quad\quad 基于矩阵分解的可扩展离散哈希}
\echapter{Scalable Discrete Matrix Factorization Hashing}
\label{chap2}


本章将具体进行\ SCRATCH 的概述、符号定义、损失函数设计、优化过程、算法扩展以及时间复杂度和相关定理分析，系统的阐述了\ SCRATCH 的理论基础。本章主要分为五个小节，分别为：概述、SCRATCH 算法、优化过程、算法分析以及本章小结。

\section{概述}
\esection{Introduction}

近年来数据的规模和种类成爆炸性增长，这对数据的存储及检索效率提出了巨大的挑战，而哈希学习提供了一种存储小检索快的解决方案，因此受到了广泛关注。哈希学习的目标是通过原始特征映射和海明空间的相似性保持来得到对应的哈希码进行检索，同时高质量的哈希码应该满足四个特性：1) 相似性保持：原始空间中相似（不相似）的样本在海明空间中也应相似（不相似）；2) 独立性：哈希码各个位之间应该互相独立，即不应存在线性或非线性关系；3) 平衡性：哈希码的每个位都有\ $50\%$ 的可能性取\ 1 或者\ $-1$；4) 离散性：很明显哈希码各位只能取\ 1 或\ $-1$ 两个值。而跨模态哈希对哈希码的质量要求有所提高，即要保证各个模态间和模态内的相似性保持要同时在训练过程中满足，且通常要将多个模态数据映射到一个公共子空间中。

为了得到满足上述特性的高质量哈希码以提高数据检索性能，我们提出了一种全新的基于矩阵分解的可扩展离散哈希方法（SCRATCH）。该方法可同时学习哈希函数和哈希码，并使用协同矩阵分解（CMF）和标签语义嵌入来实现保持模态间和模态内的相似性，借助随机正交旋转矩阵来保持训练过程中哈希码的离散特性。本文主要贡献可总结为以下几点：
\begin{itemize}
%\vspace{-3mm}
\item 提出一种全新的有监督跨模态哈希方法，从而能够有效地捕捉形态各异的数据之间的隐含联系，通过借助协同矩阵分解（CMF）和标签语义嵌入，该方法可充分利用现有的监督语义信息。
%\vspace{-3mm}
\item 提出一种迭代优化的策略来解决\ SCRATCH 中的优化问题，同时使得优化的时间复杂度与给定数据集的规模成线性关系，从而可以轻易地将其扩展到大规模数据集上。除此之外，基于迭代优化模式，SCRATCH 可以在训练过程中保持哈希码的离散特性，从而大大减少了量化误差。
\item 我们在三个常用的基准数据集上与多种当前先进的浅层和深度的跨模态哈希方法进行了对比实验，结果显示\ SCRATCH 在跨模态检索性能上均超过了所有方法，且训练速度大大提升。
\end{itemize}

\section{SCRATCH算法}
\esection{SCRATCH method}

\subsection{符号定义}
\esubsection{Notations}

假设\ $\bm{\mathcal{O}}=\{\textbf{o}_1,\textbf{o}_2,...,\textbf{o}_n\}$ 是有\ $n$ 个样本的训练集集合，其中 $\textbf{o}_i=\{\textbf{x}^{(1)}_i,...,\textbf{x}^{(m)}_i\}$ 是第\ $i$ 个样本且\ $m$ 代表模态数量。不失一般性，进一步假设每个模态的样本都是零中心化的，即\ $\sum_{i=1}^{n}\textbf{x}^{(j)}_i=\textbf{0}, j=1,...,m$；定义\ $\textbf{L} \in \mathbb{R}^{l \times n}$ 作为真实的标签矩阵，其中\ $l$ 是类别数量，如果样本\ $\textbf{x}_i$ 属于第\ $k$ 类，则\ $\textbf{L}_{ki}=1$，否则为\ 0；$\textbf{B}\in \{-1,1\}^{r \times n}$ 是待学习的哈希码矩阵，$\left \lVert \bm{\cdot} \right \rVert _F$ 代表矩阵的\ Frobenius 范数，$sgn(\bm{\cdot})$ 是逐个样本的\ $sign$ 函数，定义如下：
\begin{equation}
\label{sign}
    \begin{split}
    sgn(x) = \left\{
    \begin{aligned}
    1\  &&x>0 & ;\\
    -1\  &&x\leq0 & .
    \end{aligned}
    \right.
    \end{split}
\end{equation}

\subsection{协同矩阵分解}
\esubsection{Collective matrix factorization}

协同矩阵分解（CMF）技术的目的在于生成一种低秩向量表示，并通过近似原始数据矩阵来移除冗余信息，具体来说，通过两个秩为\ $r$ 的矩阵\ $\textbf{U} \in \mathbb{R}^{d \times r}$ 和\ $\textbf{V} \in \mathbb{R}^{r \times n}$ 的外积来近似数据矩阵\ $\textbf{X} \in \mathbb{R}^{d \times n}$，其中\ $n$ 是数据样本的数量，$d$ 是特征维度，$r$ 是隐含因子的数量。

直观上，一个多模态样本的不同模态描述的都是同一个示例，这表示各个模态共享相同的语义信息，因此我们首先假设不同模态都共享一个由协同矩阵分解（CMF）挖掘出的隐含语义空间，由此可对每个模态有以下公式表示：
\begin{equation}
\label{mf_info}
    \min_{\textbf{U}_t,\textbf{V}} \lambda_t\parallel \textbf{X}^{(t)}-\textbf{U}_t\textbf{V} \parallel^2_F+\gamma (\parallel \textbf{U}_t \parallel^2_F+ \parallel \textbf{V} \parallel^2_F),
\end{equation}
 其中\ $\textbf{X}^{(t)} \in \mathbb{R}^{d_t \times n}$ 是训练集的第\ $t$ 个模态数据，$\textbf{U}_t \in \mathbb{R}^{d_t \times r}$，$\textbf{V} \in \mathbb{R}^{r \times n}$，$d_t$ 是第\ $t$ 个模态的特征维度，$r$ 是隐含因子数量，$\lambda_t$ 和\ $\gamma$ 是平衡参数，$\Sigma_{t=1}^m \lambda_t = 1$，且矩阵\ $\textbf{V}$ 的每个列向量\ $\textbf{v}_i$ 是隐含语义空间的一个隐含因子向量，通过提取出的代表了\ $r$ 个隐含主题来代表一个数据样本。

\subsection{语义相似性保持}
\esubsection{Preserving label consistency}

同一样本的不同模态在原始空间中共享相同的语义标签，因此该样本在协同矩阵分解（CMF）挖掘出的公共隐含语义空间中的向量表示应该与原始空间包含同样的语义标签，即假设该公共隐含表示可被回归到对应的类别标签上。通过这种方式，显式的语义标签信息可被嵌入到学到的公共隐含空间中，以此来保证学到的隐含语义表示保持了和标签相同的语义一致性。为了实现上述语义保持工作，定义了公式 \ \eqref{labelinfo}如下：
\begin{equation}
\label{labelinfo}
    \min_{\textbf{G},\textbf{V}} \alpha \parallel \textbf{L}-\textbf{G}\textbf{V} \parallel^2_F+\gamma \parallel \textbf{G} \parallel^2_F,
\end{equation}
 其中\ $\textbf{L}$ 是真实标签矩阵，$\textbf{G} \in \mathbb{R}^{l \times r}$ 是映射矩阵, $l$ 是类别数量，$\alpha$ 是一个平衡参数。


\subsection{映射函数学习}
\esubsection{Learning mapping functions}

为了实现跨模态检索，首先假设一个样本的各个模态数据都可以被映射到相同的隐含公共子空间中，因此\ SCRATCH 对每个模态学习一个单独的哈希函数，从而将样本每个模态的原始数据通过各自的哈希函数映射到隐含的公共子空间中。基于上述假设及目标可得到公式 \ \eqref{mapping}：
\begin{equation}
\label{mapping}
    \min_{\textbf{P}_t,\textbf{V}} \mu \parallel \textbf{V}-\textbf{F}_t(\textbf{X}^{(t)}) \parallel^2_F + \gamma \parallel \textbf{P}_t \parallel^2_F,
\end{equation}
其中\ $\textbf{F}_t(\textbf{X}^{(t)})=\textbf{P}_t\textbf{X}^{(t)}$ 是第 $t$个模态的哈希函数，$\mu$ 是相应的平衡参数。


\subsection{哈希码学习}
\esubsection{Learning hash codes}

在通过协同矩阵分解、语义相似性保持以及多模态映射函数的结合训练得到隐含公共表示矩阵\ $\textbf{V}$ 之后，下一个难点在于如何将隐含公共表示矩阵\ $\textbf{V}$ 表示为对应的哈希码矩阵\ $\textbf{B}$，同时还要在映射过程中保持哈希码的离散特性。传统的哈希方法首先考虑的大多为先对哈希码的离散特性进行松弛，最后对得到的实值表示进行二值化，在本文中即把得到的\ $\textbf{V}$ 使用\ $sign$ 函数进行二值化后就是所需的哈希码矩阵\ $\textbf{B}$，即\ $\textbf{B} = sgn(\textbf{V})$。

然而基于松弛的哈希方法由于在训练过程中未能保持离散特性，而是选择训练结束后将实值矩阵\ $\textbf{V}$ 二值化，这会导致当隐含公共表示矩阵\ $\textbf{V}$ 某一维上的实值的分布都集中在零点周围时，原始空间中相似的数据在该维上生成的哈希码并不一致，使得松弛使用的简单的量化方式极有可能会造成相似性保持过程出现偏差，因此松弛方法通常会造成很大的量化误差。考虑到这个问题，SCRATCH 从\ ITQ\cite{gong11itq} 中获得灵感，采用了正交随机旋转矩阵来把具有离散特性的哈希码矩阵加入到训练中，从而达到最小化量化误差的目的，因此定义公式\ \eqref{hash} 来将随机正交旋转矩阵和哈希码矩阵作为一个子优化问题进行协同训练：
\begin{equation}
\label{hash}
    \begin{split}
        &\min_{\textbf{B},\textbf{R}} \parallel \textbf{B}-\textbf{R}\textbf{V} \parallel^2_F,\\
        &s.t. \ \ \ \textbf{B} \in \{-1,1\}^{r \times n}, \textbf{R}\textbf{R}^\top=\textbf{I},
    \end{split}
\end{equation}
其中\ $\textbf{R} \in \mathbb{R}^{r \times r}$ 即为随机正交旋转矩阵，$\textbf{B}$ 是所有训练样本的哈希码矩阵，$r$ 为哈希码长度。值得一提的是，如小节\ (2.3)的优化过程所示，通过引入随机正交旋转矩阵\ $\textbf{R}$，SCRATCH 在训练过程中保持了离散特性，因此可以极大程度上避免之前基于松弛技术的哈希方法所带来的巨大的量化误差。

\subsection{特征核化}
\esubsection{Feature kernelization}

为了捕捉不同模态的非线性结构，我们进一步在优化问题中采用了核化后的特征来代替原始特征，即把公式\ \eqref{mf_info} 和\ \eqref{mapping}中的原始特征矩阵$X$使用核化后矩阵\ $\bm{\phi}(\textbf{X})$ 代替，具体来说，将公式\ \eqref{mf_info} 代替为公式\ \eqref{kernel-2}，公式\ \eqref{mapping}代替为\ \eqref{mapping-2}：
\begin{equation}
\label{kernel-2}
    \min_{\textbf{U}_t,\textbf{V}} \lambda_t\parallel \bm{\phi}(\textbf{X}^{(t)})-\textbf{U}_t\textbf{V} \parallel^2_F+\gamma (\parallel \textbf{U}_t \parallel^2_F+ \parallel \textbf{V} \parallel^2_F),
\end{equation}
\vspace{-10mm}
\begin{equation}
\label{mapping-2}
    \min_{\textbf{P}_t,\textbf{V}} \mu \parallel \textbf{V}-\textbf{F}_t(\bm{\phi}(\textbf{X}^{(t)})) \parallel^2_F + \gamma \parallel \textbf{P}_t \parallel^2_F,
\end{equation}
其中$\bm{\phi}(\textbf{x})$是样本\ $\textbf{x}$ 的非线性嵌入核向量。

本文使用之前的\ SDH\cite{shen15sdh}、DCH\cite{xu17dch} 采用的\ RBF 核函数来实现非线性映射，即 $\bm{\phi}_i(\textbf{x})=exp(\frac{-\parallel \textbf{x} - \textbf{a}_i \parallel_2^{2}}{2\sigma^{2}})$，其中 $\{\textbf{a}_j\}_{j=1}^q$ 是从训练样本中随机选取的$q$个锚点，本文将其设定为 $q=500$，$\sigma$ 是核宽度，由公式\ \eqref{kernel_width} 来计算：
\begin{equation}
\label{kernel_width}
    \sigma=\frac {1}{nq}\sum_{i=1}^n\sum_{j=1}^q\parallel\textbf{x}_i-\textbf{a}_j\parallel_2
\end{equation}


\subsection{全局目标函数}
\esubsection{Overall objective function}

将公式\ \eqref{labelinfo}、\ \eqref{hash}、\ \eqref{kernel-2}和\ \eqref{mapping-2} 整合到一起后，可得到全局目标函数，如公式\ \eqref{obj}所示：
\begin{equation}
\label{obj}
    \begin{split}
        &\min_{\textbf{P}_t,\textbf{U}_t,\textbf{V}} \sum_{t=1}^m \lambda_t \parallel \bm{\phi}(\textbf{X}^{(t)})-\textbf{U}_t\textbf{V} \parallel^2_F\\
        &\ \ \ \ \ \ \ \ +\mu \sum_{t=1}^m \parallel \textbf{V}-\textbf{P}_t\bm{\phi}(\textbf{X}^{(t)}) \parallel^2_F+ \alpha \parallel \textbf{L}-\textbf{G}\textbf{V} \parallel^2_F\\
        &\ \ \ \ \ \ \ \ +\parallel \textbf{B}-\textbf{R}\textbf{V} \parallel^2_F + \gamma Re(\textbf{V}, \textbf{G}, \textbf{P}_t, \textbf{U}_t),\\
        &\ \ \ \ \ \ \ \ \ s.t. \ \ \ \sum_{t=1}^m \lambda_t=1, \textbf{B} \in \{-1,1\}^{r \times n}, \textbf{R}\textbf{R}^\top=\textbf{I},
    \end{split}
\end{equation}
其中$m$代表模态数量，$\lambda_t$、$\mu$、$\alpha$和$\gamma$均为平衡参数，用于对目标函数中的各项损失提供权重来平衡各项对于最终目标的重要性。然而，如公式\ \eqref{obj}所示，SCRATCH 模型待学习的参数很多，尽管参数增多会提高该模型的拟合能力，但却同时会增加其过拟合的风险，为了增加\ SCRATCH 模型的泛化能力，根据奥卡姆剃刀准则“如无必要，勿增实体”的指导，本文采用加入正则项的方式来限制模型参数的复杂度，通过直接作用于优化目标中来选出经验风险和模型复杂度同时较小的模型，公式\ \eqref{obj}中的$Re(\textbf{V}, \textbf{G}, \textbf{P}_t, \textbf{U}_t)$即为用于避免过拟合的正则化项，定义为各个子优化问题中正则化项之和。

为了对本文方法有更直观的理解，图\ \ref{pipeline} 展示了\ SCRATCH 的整体框架:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.97\textwidth]{figures/pipeline.eps}
\caption{SCRATCH整体框架图}
\label{pipeline}
\end{figure}


\section{优化过程}
\esection{Optimization process}

可以证明公式 \ \eqref{obj}中的优化问题与变量\ $\textbf{U}_t$、$\textbf{P}_t$、$\textbf{G}$、$\textbf{V}$、$\textbf{R}$ 和\ $\textbf{B}$ 之间是非凸关系，这导致求解该优化问题的最优解问题成为\ NP-hard 问题而难以求解。幸运的是，当固定其他矩阵变量而只优化其中一个矩阵变量时，该优化问题与该矩阵变量是成凸关系的，因此为了解决公式\ \eqref{obj}中的优化问题，SCRATCH 提出了一种迭代求解的优化策略，即每次固定其他矩阵变量来单独优化一个矩阵变量，由于可以得到该矩阵变量的闭式解，因此该优化策略的训练速度快且所需迭代次数较少，具体的优化策略如下：

%\vspace{1mm}
\textbf{第一步：固定其他矩阵变量，更新\ $\textbf{U}_t$.}

当其他矩阵变量固定时，目标函数\ \eqref{obj}可被改写为：
\begin{equation}
\label{obj-1}
    \min_{\textbf{U}_t} \lambda_t\parallel \bm{\phi}(\textbf{X}^{(t)})-\textbf{U}_t\textbf{V} \parallel^2_F+\gamma \parallel \textbf{U}_t \parallel^2_F.
\end{equation}

将公式\ \eqref{obj-1}关于\ $\textbf{U}_t$ 的导数设为零，可以得到如下关于\ $\textbf{U}_t$ 的闭式解：
\begin{equation}
\label{solveUt}
    \textbf{U}_t = \lambda_t \bm{\phi}(\textbf{X}^{(t)})\textbf{V}^{\top}(\lambda_t \textbf{V}\textbf{V}^{\top}+\gamma \textbf{I})^{-1}.
\end{equation}

%\vspace{4mm}
\textbf{第二步：固定其他矩阵变量，更新\ $\textbf{G}$.}

与$\textbf{U}_t$的求解过程相同，当其他矩阵变量固定时，公式\ \eqref{obj}可被重写为：
\begin{equation}
\label{obj-2}
    \min_{\textbf{G}} \alpha \parallel \textbf{L}-\textbf{G}\textbf{V} \parallel^2_F+\gamma \parallel \textbf{G} \parallel^2_F.
\end{equation}

同样将公式\ \eqref{obj-2}关于\ $\textbf{G}$ 的导数设为零，我们可以推导出关于\ $\textbf{G}$ 的解析解如下：
\begin{equation}
\label{solveUL}
    \textbf{G} = \alpha \textbf{L}\textbf{V}^{\top}(\alpha \textbf{V}\textbf{V}^{\top}+\gamma \textbf{I})^{-1},
\end{equation}

其中\ $\textbf{I} \in \mathbb{R}^{r \times r}$ 是一个单位矩阵。

%\vspace{1mm}
\textbf{第三步：固定其他矩阵变量，更新\ $\textbf{P}_t$.}

当其他矩阵变量固定时，公式\ \eqref{obj}可改写为：
\begin{equation}
\label{obj-3}
    \min_{\textbf{P}_t} \mu \parallel \textbf{V}-\textbf{P}_t\bm{\phi}(\textbf{X}^{(t)}) \parallel^2_F + \gamma \parallel \textbf{P}_t \parallel^2_F.
\end{equation}

通过将公式\ \eqref{obj}关于\ $\textbf{P}_t$ 的导数设为零可得：
\begin{equation}
\label{solvePt}
    \textbf{P}_t = \mu \textbf{V} \bm{\phi}(\textbf{X}^{(t)})^{\top} \Big(\mu \bm{\phi}(\textbf{X}^{(t)}) \bm{\phi}(\textbf{X}^{(t)})^{\top}+\gamma \textbf{I} \Big)^{-1}.
\end{equation}

\textbf{第四步：固定其他矩阵变量，更新\ $\textbf{V}$.}

跟前三步一样，固定其他矩阵变量同时将公式\ \eqref{obj}改写为：
\begin{equation}
\label{obj-4}
    \begin{split}
        &\min_{\textbf{V}} \sum_{t=1}^m \lambda_t \parallel \bm{\phi}(\textbf{X}^{(t)})-\textbf{U}_t\textbf{V} \parallel^2_F +\mu \sum_{t=1}^m \parallel \textbf{V}-\textbf{P}_t\bm{\phi}(\textbf{X}^{(t)}) \parallel^2_F\\
        &\ \ \ \ \ \ \ \ + \alpha \parallel \textbf{L}-\textbf{G}\textbf{V} \parallel^2_F+\parallel \textbf{B}-\textbf{R}\textbf{V} \parallel^2_F + \gamma \parallel \textbf{V} \parallel^2_F\\
    \end{split}
\end{equation}

将公式\ \eqref{obj-4}关于\ $\textbf{V}$ 的导数设为零可得：
\begin{equation}
\label{solveV}
    \begin{split}
    &\textbf{V} = \Big ( \sum_{t=1}^m \lambda_t \textbf{U}_t^{\top} \textbf{U}_t+\alpha \textbf{G}^{\top}\textbf{G} +  \textbf{R}^{\top}\textbf{R}+( m\mu+\gamma)\textbf{I} \Big) ^{-1}\bullet\\
    & \ \ \ \ \ \ \Big(\sum_{t=1}^m \big( \lambda_t \textbf{U}_t^{\top} \bm{\phi}(\textbf{X}^{(t)})+ \mu \textbf{P}_t \bm{\phi}(\textbf{X}^{(t)}) \big)+\alpha \textbf{G}^{\top}\textbf{L}+ \textbf{R}^{\top}\textbf{B} \Big). \\
    \end{split}
\end{equation}


\textbf{第五步：固定其他矩阵变量，更新\ $\textbf{R}$.}

固定其他矩阵变量后，公式\ \eqref{obj}可被改写为：
\begin{equation}
\label{solveR_eq}
    \begin{split}
        &\min_\textbf{R}\parallel \textbf{B}-\textbf{R}\textbf{V}\parallel^2_F, \\
        &s.t. \ \ \ \textbf{R}\textbf{R}^{\top}=\textbf{I}.
    \end{split}
\end{equation}

很明显这是一个经典的正交\ Procrustes 问题\ \cite{Sch1966OPP}，这个问题可借助奇异值分解（SVD）来解决，因此可得到\ $\textbf{B}\textbf{V}^\top=\textbf{S}\bm{\Omega}\bm{\widetilde{S}}^\top$，然后将公式\ \eqref{solveR_eq}展开可得：
\begin{equation}
\label{solveR-1}
    \begin{split}
        &\min_\textbf{R} \parallel\textbf{B}\parallel^2_F-2Tr(\textbf{R}\textbf{V}\textbf{B}^{\top})+\parallel\textbf{R}\parallel^2_F), \\
        &s.t. \ \ \ \textbf{R}\textbf{R}^{\top}=\textbf{I}.
    \end{split}
\end{equation}

由约束\ $\textbf{R}\textbf{R}^{\top}=\textbf{I}$ 可知：$\parallel\textbf{R}\parallel^2_F= Tr(\textbf{V}^{\top}\textbf{R}^{\top}\textbf{R}\textbf{V})=Tr(\textbf{V}\textbf{V}^{\top})$，而$\parallel\textbf{B}\parallel^2_F=Tr(\textbf{B}\textbf{B}^{\top})$=常数，同时由奇异值分解可得：$Tr(\textbf{R}\textbf{V}\textbf{B}^{\top})=Tr(\textbf{B}\textbf{V}^{\top}\textbf{R}^{\top})=Tr(\textbf{S}\bm{\Omega}\bm{\widetilde{S}}^\top\textbf{R}^{\top})=Tr(\bm{\Omega}\bm{\widetilde{S}}^\top\textbf{R}^{\top}\textbf{S})$，因此公式\ \eqref{solveR-1} 可重写为：
\begin{equation}
\label{solveR-2}
    \begin{split}
        &\max_\textbf{R} Tr(\bm{\Omega}\bm{\widetilde{S}}^\top\textbf{R}^{\top}\textbf{S}), \\
        &s.t. \ \ \ \textbf{R}\textbf{R}^{\top}=\textbf{I}.
    \end{split}
\end{equation}

由\ Von Neumann 迹不等式 \cite{mirsky1975trace}可知，两矩阵相乘的迹小于等于两矩阵各自对应的奇异值相乘之和，即$Tr\big(\bm{\Omega}\bm{\widetilde{S}}^\top\textbf{R}^{\top}\textbf{S}\big)\le \sum_{i=1}^r \sigma_i\big(\bm{\Omega}\big)\sigma_i\big(\bm{\widetilde{S}}^\top\textbf{R}^{\top}\textbf{S}\big)$，其中$\sigma_i(\textbf{X})$ 代表取矩阵$\textbf{X}$的第$i$ 个奇异值，又由于矩阵$\bm{\widetilde{S}}$和$\textbf{S}$均为酉矩阵，因此$\bm{\widetilde{S}}^\top\textbf{R}^{\top}\textbf{S}\big(\bm{\widetilde{S}}^\top\textbf{R}^{\top}\textbf{S}\big)^\top=\bm{\textbf{I}}$，故\ $\forall_i\sigma_i\big(\bm{\widetilde{S}}^\top\textbf{R}^{\top}\textbf{S}\big)=1$，可得：$Tr\big(\bm{\Omega}\bm{\widetilde{S}}^\top\textbf{R}^{\top}\textbf{S}\big)\le \sum_{i=1}^r \sigma_i\big(\bm{\Omega}\big)$，只有当\ $\textbf{R}=\bm{\textbf{S}\widetilde{S}}^\top$时等号成立，所以可以推导得出\ $\textbf{R}$ 的解析解为\ $\textbf{R}=\bm{\textbf{S}\widetilde{S}}^\top$。

%\vspace{1mm}
\textbf{第六步：固定其他矩阵变量，更新\ $\textbf{B}$.}

当其他矩阵变量固定时，公式\ \eqref{obj}重写为：
\begin{equation}
\label{solveB_eq}
    \begin{split}
        &\min_\textbf{B}\parallel \textbf{B}-\textbf{R}\textbf{V}\parallel^2_F,\\
        &s.t. \ \ \ \textbf{B}\in \{-1,1\}^{r \times n}.
    \end{split}
\end{equation}

显然可以看出上述子问题的最优解为 $\textbf{B}=sgn(\textbf{R}\textbf{V})$，且该子问题中哈希码矩阵始终保持离散特性而未借助松弛优化，极大程度上减小了量化误差，同时提高了生成的哈希码质量，使得检索性能大大提高。

为了获得优化目标\ \eqref{obj}的最优解，我们通过上述步骤迭代优化\ $\textbf{U}_t$, $\textbf{G}$, $\textbf{P}_t$, $\textbf{V}$, $\textbf{R}$ 和\ $\textbf{B}$ 直到目标收敛（目标函数值小于阈值或迭代次数达到固定次数）。为了更清晰地描述本文提出的迭代优化策略，我们在\textbf{Algorithm 1}进行了总结。
\begin{algorithm}[t]
   \caption{SCRATCH优化算法}
  \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries 输入：} 第$t$个模态的训练数据矩阵\ $\textbf{X}^{(t)}$，平衡参数$\lambda_t$、$\mu$、 $\alpha$、$\gamma$，哈希码长度$r$和总迭代次数$c$.
   \STATE {\bfseries 输出：} 哈希码矩阵\ $\textbf{B}$，映射矩阵\ $\textbf{P}_t$、$\textbf{G}$，分解矩阵\ $\textbf{U}_t$，旋转矩阵\ $\textbf{R}$ 和隐含公共表示矩阵\ $\textbf{V}$.
%   \vspace{-3mm}
   \STATE {\bfseries 流程：}\\
     1. 随机初始化矩阵变量\ $\textbf{B}$、$\textbf{R}$、$\textbf{V}$、$\textbf{U}_t$、$\textbf{G}$ 和\ $\textbf{P}_t$；\\
     2. 将$\textbf{X}^{(t)}$ 嵌入到非线性核空间中并获得核化后的特征\ $\textbf{F}_t(\textbf{X}^{(t)})$；\\
   \FOR {$i=1$ to $c$ }
   \STATE 3. 固定 $\textbf{P}_t$、$\textbf{G}$、$\textbf{V}$、$\textbf{R}$ 和 $\ \textbf{B}$，使用公式\ \eqref{solveUt}更新\ $\textbf{U}_t$ ；\\
   4. 固定 $\textbf{U}_t$、$\textbf{P}_t$、$\textbf{V}$、$\textbf{R}$ 和 $\ \textbf{B}$, 使用公式\ \eqref{solveUL}更新\ $\textbf{G}$；\\
   5. 固定 $\textbf{U}_t$、$\textbf{G}$、$\textbf{V}$、$\textbf{R}$ 和 $\ \textbf{B}$, 使用公式\ \eqref{solvePt} 更新\ $\textbf{P}_t$；\\
   6. 固定 $\textbf{U}_t$、$\textbf{G}$、$\textbf{P}_t$、$\textbf{R}$ 和 $\ \textbf{B}$, 使用公式\ \eqref{solveV} 更新\ $\textbf{V}$；\\
   7. 固定 $\textbf{U}_t$、$\textbf{G}$、$\textbf{P}_t$、$\textbf{V}$ 和 $\ \textbf{B}$, 使用公式\ $\textbf{R}=\bm{\textbf{S}\widetilde{S}}^\top$ 更新\ $\textbf{R}$；\\
   8. 固定 $\textbf{U}_t$、$\textbf{G}$、$\textbf{P}_t$、$\textbf{V}$ 和 $\ \textbf{R}$, 使用公式\ $sgn(\textbf{R}\textbf{V})$ 更新\ $\textbf{B}$；\\
   \ENDFOR
   \STATE {\bfseries 返回} $\textbf{U}_t$、$\textbf{P}_t$、$\textbf{G}$、$\textbf{V}$、$\textbf{R}$ 和 $\ \textbf{B}$.
\end{algorithmic}
\end{algorithm}


\section{算法分析}
\esection{Algorithm analysis}

\subsection{新样本扩展}
\esubsection{Out-of-sample extension}

对于一个不在训练集中的新样本来说，SCRATCH 可以快速地生成该样本的哈希码。例如，给定某个待查询多模态样本中的一个模态数据\ $\textbf{x}^{(t)}$，它对应的哈希码可由以下公式来生成：
%\vspace{-1mm}
\begin{equation}
    \textbf{b}^{(t)}=sgn\big(\textbf{R}\textbf{F}_t(\bm{\phi}(\textbf{x}^{(t)}))\big)=sgn\big(\textbf{R} \textbf{P}_t \bm{\phi}(\textbf{x}^{(t)})\big),
\end{equation}

其中 $\bm{\phi}(\textbf{x}^{(t)})$ 是样本$\textbf{x}^{(t)}$的非线性嵌入核函数。


\subsection{时间复杂度分析}
\esubsection{Time complexity analysis}

在本小节中，我们展示了\ SCRATCH 优化方法的时间复杂度与训练集规模成线性关系，从而使得\ SCRATCH 在大规模数据集上的多模态数据检索不会因为数据规模而导致训练复杂度的提升，因此具有高可扩展性和实用性。具体来说，整体的时间复杂度分别包括求解公式\ \eqref{solveUt}消耗的$O\big(c((qr + r^2)n + r^3 + qr^2)\big)$、用于公式\ \eqref{solveUL}的$O\big(c((lr + r^2)n + r^3 + lr^2)\big)$、 求解\ \eqref{solvePt}的$O\big(c((qr+q^2)n + q^3 + rq^2)\big)$、用于公式\eqref{solveV} 的O$\big(c(mqr^2+lr^2+2r^3+r(l+r+mq)n)\big)$、求$R$的解析解消耗$O\big(c(r^2n+r^3)\big)$以及求解$B$ 的$O\big(c(r^2+r)n\big)$，其中$q$是随机选取的锚点数量，$r$是哈希码长度，$l$是类别数量，$m$是模态数量，$c$ 是迭代次数。由于$r, l, q, m, c << n$，因此训练阶段所需的整体计算复杂度为$O(n(r^2+lr+q^2+mqr)c)$，可见其与训练集数据规模$n$是成线性关系的。由小节（2.4.1）可知，检索阶段生成带查询样本哈希码的时间复杂度为$O(qr)$，同时通过后面章节的实验可知，SCRATCH 的训练和检索速度都是当前跨模态哈希方法中做到速度和检索质量平衡最好的方法。

\subsection{定理分析}
\esubsection{Theorem analysis}

定理1：根据矩阵分解哈希（CMFH）\cite{ding14cmfh}，公式\ \eqref{mf_info}中的协同矩阵分解与公式\ \eqref{mapping} 中的特征映射相结合可实现相似性保持特性。

证明：根据公式\ref{obj}，对于每个模态$t$和样本$i$，通过引入重构误差$e_i^{(t)}$和$e_i^{'(t)}$，可得以下公式：
\begin{equation}
\label{proof-1}
    \begin{split}
        & x_i^{(t)}=\textbf{U}_tv_i+e_i^{(t)},\\
        & v_i=\textbf{P}_tx_i^{(t)}+e_i^{'(t)}.
    \end{split}
\end{equation}

基于公式\ \eqref{proof-1}，样本$ x_i$ 减去样本$ x_j$ 的模为：
\begin{equation}
\label{proof-2}
    \begin{split}
        &\parallel x_i^{(t)}-x_j^{(t)}-(e_i^{(t)}-e_j^{(t)})\parallel = \parallel \textbf{U}_t(v _i-v_j)\parallel,\\
        &\parallel v_i-v_j \parallel = \parallel \textbf{P}_t(x_i^{(t)}-x_j^{(t)})+(e_i^{'(t)}-e_j^{'(t)})\parallel.
    \end{split}
\end{equation}

根据模的特性：对于任意两个矩阵\ $\textbf{U}$ 和\ $\textbf{V}$，存在$\parallel\textbf{U}\textbf{V}\parallel \le \parallel\textbf{U}\parallel \parallel\textbf{V}\parallel$ 以及 $\parallel \textbf{U}+\textbf{V} \parallel \le \parallel \textbf{U} \parallel+ \parallel \textbf{V} \parallel$，可以推导出以下公式：
\begin{equation}
\label{proof-3}
    \begin{split}
        &\parallel x_i^{(t)}-x_j^{(t)}-(e_i^{(t)}-e_j^{(t)})\parallel \le \parallel \textbf{U}_t\parallel \parallel v _i-v_j \parallel,\\
        &\parallel v_i-v_j \parallel \le \parallel \textbf{P}_t(x_i^{(t)}-x_j^{(t)})\parallel+\parallel e_i^{'(t)}-e_j^{'(t)}\parallel.
    \end{split}
\end{equation}

进而可以得出如下公式结论：
\begin{equation}
\label{proof-4}
    \begin{split}
        & \parallel v_i-v_j \parallel \ge \textbf{C}1 \parallel x_i^{(t)}-x_j^{(t)}\parallel-\epsilon_1^{(t)}(i,j),\\
        & \parallel v_i-v_j \parallel \le \textbf{C}2 \parallel x_i^{(t)}-x_j^{(t)}\parallel+\epsilon_2^{(t)}(i,j).
    \end{split}
\end{equation}

其中\ $C1=max_t \frac{1}{\parallel \textbf{U}_t\parallel}$，$C2=min_t \frac{1}{\parallel\textbf{P}_t\parallel}$，除此之外，边界误差\ $\epsilon_1^{(t)}(i,j)=\frac{\parallel e_i^{(t)}-e_j^{(t)}\parallel}{\parallel \textbf{U}_t\parallel}$，$\epsilon_2^{(t)}(i,j)=\parallel e_i^{'(t)}-e_j^{'(t)}\parallel$。

定义$\epsilon_1=\{\epsilon_1^{(t)}(i,j);\forall t,i,j\}$ 和 $\ \epsilon_2=\{\epsilon_2^{(t)}(i,j);\forall t,i,j\}$ 作为边界误差集合，公式\ \eqref{proof-4}解释了$\parallel v_i-v_j \parallel$的上下边界，这意味着公式\ \eqref{mapping}中的映射函数是近似\ bi-Lipschitz 连续的。然而$\epsilon^{(t)}$的值对\ CMF 和映射函数影响很大，当\ $\textbf{X}^{(t)}$ 和\ $\textbf{V}$ 是完美分解的，则误差项$\epsilon^{(t)}$等于0，且当$x_i$与$x_j$在原始空间相似时，公式\ \eqref{proof-4} 中的上下边界误差项会保证$v_i$与$v_j$也是相近的。

综上所述，通过协同矩阵分解与特征映射相结合，最小化公式\ \eqref{mf_info} 和\ \eqref{mapping} 可实现相似性保持特性。

%定理2：\textbf{Algorithm 1}中的每次迭代过程中，公式\ \eqref{obj}中的目标函数值是单调递减的。
%
%证明：

\section{本章小结}
\esection{Summary}

本章着重讲解了\ SCRATCH 的定义、损失函数设计和优化过程，同时为了更好地理解本文提出的跨模态哈希方法，我们进一步分析了算法的扩展、时间复杂度以及相关定理，系统的阐述了\ SCRATCH 的理论基础。借助提出的损失函数和优化策略，SCRATCH 不仅保持了哈希码的相似性保持和离散特性，同时做到了线性时间内完成大规模数据集的训练，从而使得该方法具有极高的可扩展性和实用性。


